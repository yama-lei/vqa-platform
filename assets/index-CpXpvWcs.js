import{q as y,c as a,o as i,j as t,a as e,w as s,r,F as g,v as m,h,N as D,a0 as F,x as l,s as b,u as T,a1 as V}from"./vendor-DQ4wclxO.js";import{_ as H}from"./_plugin-vue_export-helper-DlAUqK2U.js";const q={class:"learning-container"},E={class:"content-section"},G={class:"resource-list"},R=["href"],M={class:"resource-desc"},j=["src","alt"],z={class:"repo-tags"},I={__name:"index",setup(S){const x=C=>{window.open(C,"_blank")},P=y([{title:"PyTorch基础",description:"学习PyTorch框架基础知识，为构建深度学习模型打下基础",icon:"Cpu",iconClass:"icon-pytorch",resources:[{name:"PyTorch官方教程",url:"https://pytorch.org/tutorials/",description:"官方文档，从入门到高级应用"},{name:"PyTorch深度学习实战",url:"https://github.com/yunjey/pytorch-tutorial",description:"实用PyTorch示例与代码"},{name:"动手学深度学习-PyTorch版",url:"https://tangshusen.me/Dive-into-DL-PyTorch/",description:"全面的PyTorch入门教程"}]},{title:"多模态融合算法",description:"学习如何将不同模态的信息（如视觉和文本）进行有效融合",icon:"Connection",iconClass:"icon-multimodal",resources:[{name:"多模态深度学习综述",url:"https://arxiv.org/abs/2105.11087",description:"多模态学习的全面综述"},{name:"ViLT: Vision-and-Language Transformer",url:"https://arxiv.org/abs/2102.03334",description:"视觉语言预训练模型"},{name:"VisualBERT介绍",url:"https://arxiv.org/abs/1908.03557",description:"用于视觉语言任务的预训练模型"},{name:"多模态融合技术博客",url:"https://lilianweng.github.io/lil-log/2020/10/29/open-domain-question-answering.html",description:"多模态融合技术详解"}]},{title:"答案生成算法",description:"学习如何基于图像内容生成准确、自然的回答",icon:"DataAnalysis",iconClass:"icon-generation",resources:[{name:"VQA调研与方法",url:"https://arxiv.org/abs/1505.00468",description:"VQA研究的开创性论文"},{name:"BLIP-2: Bootstrapping",url:"https://arxiv.org/abs/2301.12597",description:"视觉语言预训练与生成"},{name:"多模态大型语言模型",url:"https://arxiv.org/abs/2302.00923",description:"LLaMA模型与多模态应用"}]},{title:"计算机视觉基础",description:"学习图像处理与分析的基础知识",icon:"Monitor",iconClass:"icon-vision",resources:[{name:"斯坦福CS231n课程",url:"http://cs231n.stanford.edu/",description:"卷积神经网络视觉识别"},{name:"OpenCV入门指南",url:"https://docs.opencv.org/master/d9/df8/tutorial_root.html",description:"图像处理库入门"},{name:"YOLO目标检测",url:"https://github.com/ultralytics/yolov5",description:"高效目标检测算法"}]},{title:"自然语言处理",description:"学习文本处理与理解的核心技术",icon:"Reading",iconClass:"icon-nlp",resources:[{name:"Transformer模型详解",url:"https://arxiv.org/abs/1706.03762",description:"注意力机制原理论文"},{name:"BERT预训练模型",url:"https://arxiv.org/abs/1810.04805",description:"双向Transformer表示"},{name:"HuggingFace Transformers",url:"https://huggingface.co/docs/transformers/index",description:"Transformers库使用指南"}]},{title:"VQA数据集与评估",description:"学习VQA任务的常用数据集与评估指标",icon:"PictureFilled",iconClass:"icon-dataset",resources:[{name:"VQA数据集",url:"https://visualqa.org/",description:"视觉问答标准数据集"},{name:"COCO-QA数据集",url:"https://github.com/ramprs/grad-cam/",description:"基于COCO的问答数据"},{name:"GQA平衡数据集",url:"https://cs.stanford.edu/people/dorarad/gqa/",description:"视觉问答推理数据集"},{name:"VQA评估指标",url:"https://github.com/GT-Vision-Lab/VQA",description:"VQA任务评估工具"}]}]),A=y([{name:"Hugging Face",description:"提供最先进的NLP和CV模型库，包含丰富的预训练模型和工具",logo:"https://huggingface.co/front/assets/huggingface_logo.svg",url:"https://huggingface.co/",tags:["预训练模型","开源社区","Transformers"]},{name:"GitHub",description:"开源代码托管平台，可以找到各种VQA相关项目和实现",logo:"https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png",url:"https://github.com/",tags:["代码托管","开源社区","版本控制"]},{name:"PyTorch Hub",description:"PyTorch提供的预训练模型库，包含众多视觉和语言模型",logo:"https://pytorch.org/assets/images/pytorch-logo.png",url:"https://pytorch.org/hub/",tags:["PyTorch","预训练模型","深度学习"]},{name:"Papers With Code",description:"将论文与开源实现关联的平台，方便查找最新研究及代码",logo:"https://paperswithcode.com/static/logo.png",url:"https://paperswithcode.com/",tags:["研究论文","代码实现","基准测试"]}]),k=y([{title:"基础知识",description:"Python、PyTorch和深度学习基础"},{title:"计算机视觉",description:"CNN、目标检测与图像分割"},{title:"自然语言处理",description:"Transformer、BERT与文本理解"},{title:"多模态融合",description:"视觉与语言特征融合方法"},{title:"VQA模型实现",description:"构建完整的视觉问答系统"}]);return(C,n)=>{const c=r("el-col"),u=r("el-row"),_=r("el-icon"),Q=r("el-collapse-item"),w=r("el-collapse"),f=r("el-card"),L=r("el-tag"),B=r("el-button"),N=r("el-step"),O=r("el-steps");return i(),a("div",q,[n[5]||(n[5]=t("div",{class:"header-area"},[t("div",{class:"header-content"},[t("h1",{class:"title"},"学习空间"),t("p",{class:"description"},"[声明：本页面全部由ai生成]")])],-1)),t("div",E,[e(u,{gutter:20},{default:s(()=>[e(c,{span:24},{default:s(()=>n[0]||(n[0]=[t("div",{class:"section-header"},[t("h2",null,"学习主题"),t("div",{class:"section-divider"})],-1)])),_:1})]),_:1}),e(u,{gutter:20,class:"learning-cards"},{default:s(()=>[(i(!0),a(g,null,m(P.value,(o,d)=>(i(),h(c,{xs:24,sm:12,lg:8,key:d},{default:s(()=>[e(f,{class:"learning-card",shadow:"hover"},{default:s(()=>[t("div",{class:D(["category-icon",o.iconClass])},[e(_,null,{default:s(()=>[(i(),h(F(o.icon)))]),_:2},1024)],2),t("h3",null,l(o.title),1),t("p",null,l(o.description),1),e(w,null,{default:s(()=>[e(Q,{title:`查看${o.resources.length}个资源`},{default:s(()=>[t("ul",G,[(i(!0),a(g,null,m(o.resources,(p,v)=>(i(),a("li",{key:v},[t("a",{href:p.url,target:"_blank",class:"resource-link"},[e(_,null,{default:s(()=>[e(T(V))]),_:1}),b(" "+l(p.name),1)],8,R),t("span",M,l(p.description),1)]))),128))])]),_:2},1032,["title"])]),_:2},1024)]),_:2},1024)]),_:2},1024))),128))]),_:1}),e(u,{gutter:20,class:"mt-40"},{default:s(()=>[e(c,{span:24},{default:s(()=>n[1]||(n[1]=[t("div",{class:"section-header"},[t("h2",null,"代码仓库"),t("div",{class:"section-divider"})],-1)])),_:1})]),_:1}),e(u,{gutter:20,class:"repo-cards"},{default:s(()=>[(i(!0),a(g,null,m(A.value,(o,d)=>(i(),h(c,{xs:24,sm:12,md:8,lg:6,key:d},{default:s(()=>[e(f,{class:"repo-card",shadow:"hover"},{default:s(()=>[t("img",{src:o.logo,class:"repo-logo",alt:o.name},null,8,j),t("h3",null,l(o.name),1),t("p",null,l(o.description),1),t("div",z,[(i(!0),a(g,null,m(o.tags,(p,v)=>(i(),h(L,{key:v,size:"small",class:"repo-tag"},{default:s(()=>[b(l(p),1)]),_:2},1024))),128))]),e(B,{type:"primary",onClick:p=>x(o.url),class:"repo-button"},{default:s(()=>[e(_,null,{default:s(()=>[e(T(V))]),_:1}),n[2]||(n[2]=b(" 访问仓库 "))]),_:2},1032,["onClick"])]),_:2},1024)]),_:2},1024))),128))]),_:1}),e(u,{gutter:20,class:"mt-40"},{default:s(()=>[e(c,{span:24},{default:s(()=>n[3]||(n[3]=[t("div",{class:"section-header"},[t("h2",null,"推荐学习路径"),t("div",{class:"section-divider"})],-1)])),_:1})]),_:1}),e(u,{gutter:20},{default:s(()=>[e(c,{span:24},{default:s(()=>[e(f,{class:"learning-path-card"},{default:s(()=>[e(O,{active:1,"finish-status":"success",simple:""},{default:s(()=>[(i(!0),a(g,null,m(k.value,(o,d)=>(i(),h(N,{key:d,title:o.title,description:o.description},null,8,["title","description"]))),128))]),_:1}),n[4]||(n[4]=t("div",{class:"learning-path-detail"},[t("h3",null,"从入门到精通的VQA学习指南"),t("p",null,"视觉问答(VQA)是一个充满挑战的跨领域研究方向，结合了计算机视觉和自然语言处理的技术。按照上述路径学习，可循序渐进地掌握相关知识和技能。")],-1))]),_:1})]),_:1})]),_:1})])])}}},W=H(I,[["__scopeId","data-v-499a0b69"]]);export{W as default};
